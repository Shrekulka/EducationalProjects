Код предназначен для анализа веб-страницы на русской Википедии, извлечения внутренних и внешних ссылок и отображения их 
в удобочитаемом формате. Он использует библиотеки requests, BeautifulSoup и urllib.parse. В коде реализованы три решения
для выполнения этой задачи.

### Первое решение:

1. Отправляет GET-запрос на веб-страницу.
2. Парсит HTML-код с помощью BeautifulSoup.
3. Извлекает все ссылки (<a> теги) на странице.
4. Разделяет ссылки на внутренние и внешние, проверяя наличие домена в URL.
5. Сохраняет ссылки в словарь `defaultdict(set)`, где ключи — это типы ссылок (внутренние или внешние), а значения — 
   множества уникальных ссылок.
6. Печатает список внутренних и внешних ссылок, декодируя URL-кодированные символы.

### Второе решение:

1. Отправляет GET-запрос на веб-страницу.
2. Парсит HTML-код с помощью BeautifulSoup.
3. Извлекает все ссылки (<a> теги) на странице.
4. Разделяет ссылки на внутренние и внешние, проверяя наличие домена в URL.
5. Хранит внутренние и внешние ссылки в отдельных множествах.
6. Печатает список внутренних и внешних ссылок, декодируя URL-кодированные символы.

### Третье решение:

1. Отправляет GET-запрос на веб-страницу.
2. Парсит HTML-код с помощью BeautifulSoup.
3. Извлекает все ссылки (<a> теги) на странице.
4. Разделяет ссылки на внутренние и внешние, проверяя наличие домена в URL.
5. Использует функции для получения содержимого страницы, извлечения ссылок и их вывода.
6. Печатает список внутренних и внешних ссылок, декодируя URL-кодированные символы.

Все три решения выполняют схожие задачи по разделению ссылок на внутренние и внешние и отображению результатов в 
читаемом формате, но используют разные подходы для хранения и обработки информации.
