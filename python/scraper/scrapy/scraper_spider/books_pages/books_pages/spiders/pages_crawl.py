# scraper_spider/books_pages/books_pages/spiders/pages_crawl.py

from urllib.parse import urljoin

from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


# Определяем класс паука, наследуясь от CrawlSpider
class PagesCrawlSpider(CrawlSpider):
    # Задаем имя паука
    name = "pages_crawl"
    # Указываем домены, с которых разрешено собирать данные
    allowed_domains = ["books.toscrape.com"]
    # Определяем начальные URL, с которых паук начнет работу
    start_urls = ["https://books.toscrape.com"]

    # Определяем правила для навигации по сайту
    ####################################################################################################################
    rules = (
        # Правило 1: Переход на страницу отдельной книги
        Rule(
            # Извлечение ссылок на книги
            LinkExtractor(restrict_xpaths="//article[@class='product_pod']/h3/a"),
            # Это выражение находит все ссылки (<a>) внутри заголовков (<h3>),
            # которые находятся внутри элементов <article> с классом 'product_pod'.
            # Оно используется для нахождения ссылок на отдельные страницы книг в каталоге.

            # Вызываем метод parse_item для обработки страницы книги
            callback="parse_item",
            # Разрешаем следовать по извлеченным ссылкам
            follow=True
        ),

        # Правило 2: Переход на следующую страницу каталога
        Rule(
            # Извлекаем ссылку "next" для перехода на следующую страницу
            LinkExtractor(restrict_xpaths="//li[@class='next']/a")
            # Это выражение находит ссылку (<a>) внутри элемента списка (<li>) с классом 'next'.
            # Оно используется для нахождения ссылки "Следующая страница" в пагинации.
        )
    )
    ####################################################################################################################

    # Метод для обработки страницы отдельной книги
    ####################################################################################################################
    def parse_item(self, response):
        # Создаем словарь для хранения данных о книге
        item = {}

        # Извлекаем название книги
        item["title"] = response.xpath("//div[contains(@class, 'product_main')]/h1/text()").get()
        # Это выражение находит текст внутри заголовка <h1>, который находится
        # внутри <div> с классом, содержащим 'product_main'.
        # Оно извлекает название книги со страницы книги.

        # Извлекаем цену книги
        item["price"] = response.xpath("//div[contains(@class, 'product_main')]/p[@class = 'price_color']/text()").get()
        # Это выражение находит текст внутри параграфа <p> с классом 'price_color',
        # который находится внутри <div> с классом, содержащим 'product_main'.
        # Оно извлекает цену книги.

        # Извлекаем относительный URL изображения книги
        relative_image_url = response.xpath(
            "//div[@id='product_gallery']/div[@class = 'thumbnail']/div[@class = 'carousel-inner']/div[contains(@class, 'item')]/img/@src"
        ).get()
        # Это сложное выражение находит атрибут 'src' тега <img>, который находится
        # в цепочке вложенных элементов: <div id='product_gallery'> -> <div class='thumbnail'> ->
        # <div class='carousel-inner'> -> <div> (с классом, содержащим 'item').
        # Оно извлекает URL изображения книги.

        # Преобразуем относительный URL изображения в абсолютный
        item["image"] = urljoin(response.url, relative_image_url)

        # Извлекаем описание книги
        item["description"] = response.xpath("//div[@id='product_description']/../p/text()").get()
        # Это выражение находит текст в параграфе <p>, который является соседним элементом
        # (на том же уровне) с <div> с id 'product_description'.
        # Оно извлекает описание книги.

        # Извлекаем UPC (универсальный код продукта) книги
        item["upc"] = response.xpath("//th[contains(text(), 'UPC')]/../td/text()").get()
        # Это выражение находит текст в ячейке таблицы <td>, которая находится
        # в той же строке, что и заголовок <th>, содержащий текст 'UPC'.
        # Оно извлекает универсальный код продукта (UPC) книги.

        # Возвращаем собранные данные о книге
        return item
    ####################################################################################################################
